\section{Mininet}
\label{sec:exemulmininet}

Mininet~\cite{mininetnetworklaptop} is a network emulator created by researchers at Stanford University, intended to be lightweight, and allow, as the title of the paper that presented it for the first time says, \textquote{rapid prototyping for Software-Defined Networks.}

Like essentially every emulator able to run on commodity PCs, standalone, Mininet makes use of virtualization.
It was created specifically with that in mind, as an alternative to the limited simulators, but also expensive testbeds, that require a layer of real software, even if provided ``as a service'' to the users.
It also has had from the beginning heavy concerns about performance and the ability to run large topologies on the researchers' machine.

Mininet operates exclusively on Linux hosts and does everything with the low-level primitives of LXC (Linux Containers).
This enabled very large topologies to be run without much effort from the host they're supported by.

The kinds of nodes Mininet supports can't be dissociated from the fact that is was developed with \gls{sdn} in mind.
They are:
\begin{description}
	\item[hosts] corresponding to network namespaces, containers, consisting in processes with exclusive ownership of interfaces, ports, and routing tables;
	\item[switches] these are OpenFlow ``dumb'' switches, \textbf{not} ``conventional'' layer-2 switches, which can run in kernel-space or user-space and be configured by the Mininet infrastructure;
	\item[controllers] which can be on the ``virtual'' topology, but also be proper nodes in a deployed \gls{sdn}, as long as the host where the other nodes are running has IP connectivity to the rest of the network.
\end{description}

There is a command-line utility to interact with Mininet, which serves to add and remove, and also start and stop, topology nodes, and access their shells.
It is also possible to automate these actions and declare shareable topologies using Mininet's Python \gls{api}.

It is out of the scope of this dissertation studying in details what can be done with the \gls{sdn} approach e.g. in terms of programming the control plane of switches, as well as how that is done, since the focus from the beginning is to reproduce experiments currently runnable with standard, layer-2 switches and conventional IP routers.

There have been attempts of using Mininet, either using its plain version as starting point~\cite{mininetquagga,mininetospfbgp} or adapting the software~\cite{mininext} to leverage the container orchestration and automation of the setup of virtual links between those containers, as a way to create topologies with standard routers.
This can be achieved by installing routing daemons (e.g. Quagga~\cite{quagga}) in host nodes with multiple interfaces, so that the control-plane part of standard routers (updating the containers' routing tables according to a distributed protocol like OSPF or BGP) is responsability of the node itself.
However, installing Quagga demands a manual process of adding the source \emph{tarball} into the containers, and then compiling and installing from source, plus configuring, which demands some Python manual scripting.
On the other hand, out-of-the-box solutions, like miniNeXT, aren't viable either, as its repository doesn't receive a single commit since 2017, which means that it is probably in an unmaintained state.

Fortunately, there are other solutions, some able to run in more platforms (any host/OS supporting Docker out-of-the-box), which provide good, documented support for conventional open-source switches/routers (e.g. Quagga), maintaining the positive aspects of Mininet like declarative, scriptable topologies, and lightweight virtualization for large topologies.

% end of section exemulmininet
